{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0', render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 20\n",
    "\n",
    "bins_position = np.linspace(-1.2, 0.6, num_bins)\n",
    "bins_velocity = np.linspace(-0.07, 0.07, num_bins)\n",
    "\n",
    "def discretize (obs):\n",
    "    discretized_position = bins_position[np.digitize(obs[0], bins_position)]\n",
    "    discretized_velocity = bins_velocity[np.digitize(obs[1], bins_velocity)]\n",
    "    return tuple((discretized_position, discretized_velocity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The agent - based on Q-learning**\n",
    "\n",
    "To ensure that the agents explores the environment,the solution applied here is the ``epsilon-greedy`` strategy: a random action is picked with the percentage ``epsilon`` and the greedy action (currently valued as the best) with  ``1 - epsilon``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_bins: int,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        # Initialize a Reinforcement Learning agent with an empty dictionary of state-action values (q_values), a learning rate and an epsilon\n",
    "        \n",
    "        self.num_bins = num_bins\n",
    "        self.q_values = {}\n",
    "\n",
    "        for i in range(self.num_bins):\n",
    "            for j in range(self.num_bins):\n",
    "                self.q_values[(bins_position[i], bins_velocity[j])] = np.zeros(env.action_space.n)\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        # Returns the best action with probability (1 - epsilon)\n",
    "        # otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The testing settings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 10_000\n",
    "start_epsilon = 0.1 # start_epsilon is very low since it's only for testing purposes\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2) \n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = QAgent(\n",
    "    num_bins=num_bins,\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the json-file:**\n",
    "\n",
    "Deserialize the json file and convert the data formats back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('q_table.pkl', 'rb') as f:\n",
    "    data_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-1.2, -0.07) : [0. 0. 0.]\n",
      "(-1.2, -0.06263157894736843) : [0. 0. 0.]\n",
      "(-1.2, -0.05526315789473685) : [0. 0. 0.]\n",
      "(-1.2, -0.04789473684210527) : [0. 0. 0.]\n",
      "(-1.2, -0.04052631578947369) : [0. 0. 0.]\n",
      "(-1.2, -0.03315789473684211) : [0. 0. 0.]\n",
      "(-1.2, -0.02578947368421053) : [0. 0. 0.]\n",
      "(-1.2, -0.01842105263157895) : [0. 0. 0.]\n",
      "(-1.2, -0.01105263157894737) : [0. 0. 0.]\n",
      "(-1.2, -0.00368421052631579) : [0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for key, value in list(data_loaded.items())[:10]:\n",
    "    print(key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the Q-table of the agent based on the loaded data\n",
    "agent.q_values = data_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminated = False\n",
    "obs, info = env.reset()\n",
    "obs = discretize(obs)\n",
    "while not terminated:\n",
    "    env.render()\n",
    "    action = agent.get_action(obs)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    obs = discretize(obs)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Source for Q-Learning implemetation:** \n",
    "\n",
    "https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/#sphx-glr-tutorials-training-agents-blackjack-tutorial-py (02-06-2024)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.14 ('game')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9c7695616f84c207d2f03371909ec467fa192a97182d983036f60fec5165cb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
